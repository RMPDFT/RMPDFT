{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RMPDFT/RMPDFT/blob/main/C%C3%B3digo_com_Simula%C3%A7%C3%A3o_de_IA_Scraping_MPDFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# FILE: anomaly_detector.py (NOVO ARQUIVO)\n",
        "# Módulo SIMULADO para análise de anomalias e conformidade com IA.\n",
        "# Em um projeto real, este módulo conteria os modelos de Machine Learning\n",
        "# e NLP treinados, conforme o plano de projeto.\n",
        "# ==============================================================================\n",
        "import random\n",
        "import logging\n",
        "\n",
        "def analisar_contrato_com_ia(contrato_data):\n",
        "    \"\"\"\n",
        "    SIMULAÇÃO de uma análise de IA em um contrato.\n",
        "\n",
        "    Esta função recebe um dicionário com dados de um contrato e retorna\n",
        "    uma pontuação de risco e uma lista de flags (sinais de alerta).\n",
        "\n",
        "    Args:\n",
        "        contrato_data (dict): Dicionário contendo informações do contrato\n",
        "                              (ex: objeto, valor, contratado, etc.).\n",
        "\n",
        "    Returns:\n",
        "        dict: Um dicionário com 'risk_score' e 'risk_flags'.\n",
        "    \"\"\"\n",
        "    logging.info(f\"IA: Analisando contrato '{contrato_data.get('numero_contrato', 'N/A')}'...\")\n",
        "\n",
        "    flags = []\n",
        "    score = 0\n",
        "\n",
        "    objeto = contrato_data.get('objeto', '').lower()\n",
        "    situacao = contrato_data.get('situacao', '').lower()\n",
        "\n",
        "    # Simulação de regras baseadas em palavras-chave e padrões, conforme Fase 2 do plano.\n",
        "    if \"dispensa de licitação\" in objeto or \"inexigibilidade\" in objeto:\n",
        "        flags.append(\"Verificar enquadramento da contratação direta\")\n",
        "        score += 25\n",
        "\n",
        "    if \"aditivo\" in objeto or \"termo aditivo\" in objeto:\n",
        "        flags.append(\"Contrato com aditivos, verificar histórico e limites\")\n",
        "        score += 20\n",
        "\n",
        "    if \"emergencial\" in objeto or \"calamidade\" in objeto:\n",
        "        flags.append(\"Contratação emergencial, requer justificativa robusta\")\n",
        "        score += 30\n",
        "\n",
        "    # Simulação de análise de situação\n",
        "    if \"suspenso\" in situacao or \"rescindido\" in situacao:\n",
        "        flags.append(f\"Atenção: Contrato com status '{situacao.upper()}'\")\n",
        "        score += 40\n",
        "\n",
        "    # Adiciona um pouco de aleatoriedade para simular a incerteza de um modelo real\n",
        "    score += random.randint(0, 15)\n",
        "\n",
        "    # Normaliza o score para ficar entre 0 e 100\n",
        "    risk_score = min(score, 100)\n",
        "\n",
        "    if not flags:\n",
        "        flags.append(\"Nenhuma anomalia óbvia detectada pela simulação.\")\n",
        "\n",
        "    return {\n",
        "        \"risk_score\": risk_score,\n",
        "        \"risk_flags\": flags\n",
        "    }\n",
        "\n",
        "# ==============================================================================\n",
        "# FILE: scraper_mpdft.py\n",
        "# VERSÃO 4: Sem alterações neste arquivo. Mantido para integridade do projeto.\n",
        "# ==============================================================================\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from urllib.parse import urljoin\n",
        "import os\n",
        "import re\n",
        "\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "    'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7'\n",
        "}\n",
        "BASE_URL = \"https://www.mpdft.mp.br/transparencia/index.php\"\n",
        "\n",
        "def clean_url(url_string):\n",
        "    if not isinstance(url_string, str): return url_string\n",
        "    match = re.search(r'https?://[^\\s\")\\]]+', url_string)\n",
        "    return match.group(0) if match else url_string.strip()\n",
        "\n",
        "def fetch_page_content(url, params=None, retries=3, delay=5):\n",
        "    cleaned_url = clean_url(url)\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = requests.get(cleaned_url, headers=HEADERS, params=params, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            response.encoding = response.apparent_encoding\n",
        "            return BeautifulSoup(response.text, 'html.parser')\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logging.error(f\"Erro na requisição para {cleaned_url} (tentativa {attempt + 1}/{retries}): {e}\")\n",
        "            if attempt < retries - 1: time.sleep(delay)\n",
        "    logging.critical(f\"Falha ao buscar a página {cleaned_url} após {retries} tentativas.\")\n",
        "    return None\n",
        "\n",
        "def parse_contratos_list(soup):\n",
        "    contratos_data = []\n",
        "    if not soup: return contratos_data\n",
        "    content_div = soup.find('div', class_='conteudo')\n",
        "    if not content_div: return contratos_data\n",
        "    main_table = content_div.find('table')\n",
        "    if not main_table: return contratos_data\n",
        "    tbody = main_table.find('tbody')\n",
        "    if not tbody: return contratos_data\n",
        "    contrato_items = tbody.find_all('tr')\n",
        "    for item in contrato_items:\n",
        "        cols = item.find_all('td')\n",
        "        if len(cols) < 5: continue\n",
        "        try:\n",
        "            num_contrato_tag = cols[0].find('a')\n",
        "            num_contrato = num_contrato_tag.get_text(strip=True) if num_contrato_tag else cols[0].get_text(strip=True)\n",
        "            link_contrato = urljoin(BASE_URL, num_contrato_tag['href']) if num_contrato_tag and num_contrato_tag.has_attr('href') else None\n",
        "            contratado_info = cols[2].get_text(strip=True).split('CNPJ:')\n",
        "            contrato = {\n",
        "                'numero_contrato': num_contrato,\n",
        "                'data_publicacao': cols[1].get_text(strip=True),\n",
        "                'contratado_nome': contratado_info[0].strip(),\n",
        "                'contratado_cnpj': contratado_info[1].strip() if len(contratado_info) > 1 else 'N/D',\n",
        "                'objeto': cols[3].get_text(strip=True),\n",
        "                'situacao': cols[4].get_text(strip=True),\n",
        "                'link_detalhes': link_contrato,\n",
        "            }\n",
        "            contratos_data.append(contrato)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Erro ao parsear linha de contrato: {e}\")\n",
        "    return contratos_data\n",
        "\n",
        "def get_all_contratos_mpdft(ano):\n",
        "    all_contratos = []\n",
        "    pagina = 1\n",
        "    while True:\n",
        "        params = {'item': 'contratos', 'resp': 'CONTRATOS', 'ano_contrato': str(ano), 'pagina': pagina}\n",
        "        logging.info(f\"Buscando contratos - Ano: {ano}, Página: {pagina}\")\n",
        "        soup = fetch_page_content(BASE_URL, params=params)\n",
        "        if not soup: break\n",
        "        contratos_pagina = parse_contratos_list(soup)\n",
        "        if not contratos_pagina:\n",
        "            logging.info(f\"Nenhuma outra página de contratos encontrada para o ano {ano}.\")\n",
        "            break\n",
        "        all_contratos.extend(contratos_pagina)\n",
        "        pagina += 1\n",
        "        time.sleep(1)\n",
        "    return all_contratos\n",
        "\n",
        "# ==============================================================================\n",
        "# FILE: utils.py\n",
        "# VERSÃO 4: Sem alterações neste arquivo.\n",
        "# ==============================================================================\n",
        "\n",
        "DATA_DIR = \"data\"\n",
        "\n",
        "def save_to_csv(dataframe, filename):\n",
        "    if not os.path.exists(DATA_DIR):\n",
        "        os.makedirs(DATA_DIR)\n",
        "    filepath = os.path.join(DATA_DIR, filename)\n",
        "    dataframe.to_csv(filepath, index=False, encoding='utf-8-sig')\n",
        "    logging.info(f\"DataFrame salvo com sucesso em '{filepath}'\")\n",
        "\n",
        "# ==============================================================================\n",
        "# FILE: main.py\n",
        "# Ponto de entrada principal\n",
        "# VERSÃO 4: Integrada a chamada ao módulo de IA simulado.\n",
        "# ==============================================================================\n",
        "\n",
        "from datetime import datetime\n",
        "# Importa o novo módulo de IA, que agora faz parte do mesmo script para simplicidade\n",
        "# Em um projeto maior, estaria em anomaly_detector.py\n",
        "\n",
        "def run_contratos_scraper_com_analise(anos):\n",
        "    \"\"\"Executa o scraper de contratos e depois a análise de IA.\"\"\"\n",
        "    logging.info(f\"--- INICIANDO SCRAPER DE CONTRATOS PARA ANOS: {anos} ---\")\n",
        "    todos_contratos = []\n",
        "    for ano in anos:\n",
        "        todos_contratos.extend(get_all_contratos_mpdft(ano))\n",
        "\n",
        "    if not todos_contratos:\n",
        "        logging.info(\"Nenhum contrato encontrado.\")\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(todos_contratos)\n",
        "\n",
        "    # --- ETAPA DE ANÁLISE COM IA (FASE 5 DO PLANO) ---\n",
        "    logging.info(\"--- INICIANDO ANÁLISE DE CONTRATOS COM MÓDULO DE IA (SIMULADO) ---\")\n",
        "\n",
        "    # Converte o DataFrame para uma lista de dicionários para iterar\n",
        "    contratos_list = df.to_dict('records')\n",
        "    analysis_results = [analisar_contrato_com_ia(contrato) for contrato in contratos_list]\n",
        "\n",
        "    # Adiciona os resultados da análise ao DataFrame original\n",
        "    df_analysis = pd.DataFrame(analysis_results)\n",
        "    df['risk_score'] = df_analysis['risk_score']\n",
        "    # Converte a lista de flags em uma string legível\n",
        "    df['risk_flags'] = df_analysis['risk_flags'].apply(lambda x: ', '.join(x))\n",
        "\n",
        "    # Reordena colunas para melhor visualização, trazendo a análise para a frente\n",
        "    cols_to_move = ['risk_score', 'risk_flags', 'numero_contrato', 'objeto', 'contratado_nome']\n",
        "    df = df[cols_to_move + [col for col in df.columns if col not in cols_to_move]]\n",
        "\n",
        "    # Salva o resultado final enriquecido\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    save_to_csv(df, f\"contratos_analisados_com_ia_{timestamp}.csv\")\n",
        "    logging.info(f\"Análise de IA concluída. Resultados salvos em 'data/contratos_analisados_com_ia_{timestamp}.csv'\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Configuração da execução\n",
        "    ANOS_PARA_BUSCAR = [datetime.now().year] # Busca apenas o ano corrente\n",
        "\n",
        "    # Executa o fluxo principal de coleta e análise\n",
        "    run_contratos_scraper_com_analise(ANOS_PARA_BUSCAR)\n",
        "\n",
        "    logging.info(\"Processo principal concluído.\")"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "id": "llS-XzIahC4x"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}